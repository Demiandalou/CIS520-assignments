{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw11_CIS520.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJf1AdKCxZaS"
      },
      "source": [
        "# Homework 11: Coding\n",
        "\n",
        "**Due December 10th, 11:59pm.**\n",
        "\n",
        "**This is a group assignment.**\n",
        "\n",
        "**Submit the link for hw11.ipynb to Gradescope (you may submit as many times as you'd like before the deadline). Note that there is no autograder.**\n",
        "\n",
        "**Please use Google Colab to maintain consistency across the class.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVsgePskSmmM"
      },
      "source": [
        "# 1 RNN & GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44on_G7jSsnQ"
      },
      "source": [
        "## 1.1 Char-RNN Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7KrOs93oNxR"
      },
      "source": [
        "For this problem we use the UCIrvine name-gender dataset. Please upload the dataset to Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_06HSlqSoHNm"
      },
      "source": [
        "import csv\n",
        "file = open('name_gender_dataset.csv')\n",
        "csvreader = csv.reader(file)\n",
        "header = []\n",
        "header = next(csvreader)\n",
        "data = []\n",
        "for row in csvreader:\n",
        "  data.append(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbkgNltto3cO"
      },
      "source": [
        "Run the cell below to see the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvZcwZS3EbPT"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKoBrJjpC11L"
      },
      "source": [
        "Partitioning the data into training / dev / testing sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR9SJtlXzQtz"
      },
      "source": [
        "train = data[:97000]\n",
        "dev = data[97000:100000]\n",
        "test = data[100000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "CNS4OZXxMZ-A"
      },
      "source": [
        "#Setup environment\n",
        "!pip3 install scikit-learn\n",
        "!pip install \"wheel==0.34.2\"\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "!pip3 install https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torch torchvision\n",
        "  \n",
        "import torch\n",
        "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "qIJxbqMDNTTl"
      },
      "source": [
        "#Verify CUDA acceleration should print cuda:0\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9xnthBaEzqy"
      },
      "source": [
        "Initializing helper constants:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RDyXc6KDW_s"
      },
      "source": [
        "# Reference: pytorch document on Char-RNN setup\n",
        "\n",
        "import codecs\n",
        "import math\n",
        "import random\n",
        "import string\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "genders = ['M', 'F'] # note this binary situation is only for the classification purpose\n",
        "all_letters = string.ascii_letters + \".,;'-#0123456789()&/\" + '\"'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7jAkyIREnX-"
      },
      "source": [
        "###1.1.1 Model Construction (10')\n",
        "In this part, construct a RNN / LSTM (of your choice) model that fits the input and output of the classification task. Note that the RNN / LSTM is recurrent - what does that imply of the dimensions of the input? Hint: think of an individual character and how many possibilities it would have. Also think about one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "NbOusBLKPsrx"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "'''\n",
        "Please add default values for all the parameters of __init__.\n",
        "'''\n",
        "class CharRNNClassify(nn.Module):\n",
        "    def __init__(self, input_size=len(all_letters), hidden_size=128, output_size=len(genders)):\n",
        "        #TODO: implement\n",
        "\n",
        "    def forward(self, input, hc):\n",
        "        #TODO: implement\n",
        "\n",
        "    def init_hidden(self):\n",
        "        #TODO: implement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7luZjVvFgZl"
      },
      "source": [
        "###1.1.2 Helper Functions (5'*2=10')\n",
        "Here are some useful helper functions; please implement the trainOneEpoch and run methods:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "kwB5RzvfOGr_"
      },
      "source": [
        "\"\"\"\n",
        "Convert data into np array - note gender is converted into index\n",
        "\"\"\"\n",
        "def readData(data):\n",
        "  X = []\n",
        "  y = []\n",
        "  for name in data:\n",
        "    X.append(name[0])\n",
        "    y.append(0 if name[1] == 'M' else 1) # M: 0, F: 1\n",
        "  return np.array(X), np.array(y)\n",
        "\n",
        "'''\n",
        "Convert a line/word to a pytorch tensor of numbers\n",
        "Refer the tutorial in the spec\n",
        "Return: A tensor corresponding to the given line\n",
        "'''\n",
        "def name_to_tensor(name):\n",
        "  n_letters = len(all_letters)\n",
        "  tensor = torch.zeros(len(name), 1, n_letters)\n",
        "  for index, letter in enumerate(name):\n",
        "    tensor[index][0][all_letters.find(letter)] = 1\n",
        "  return tensor\n",
        "\n",
        "'''\n",
        "Returns the gender of the output from the neural network\n",
        "Input: Output of the neural networks (class probabilities)\n",
        "'''\n",
        "def category_from_output(output):\n",
        "    _, top_indice = output.topk(1)\n",
        "    gender_index = top_indice[0].item()\n",
        "    return genders[gender_index], gender_index\n",
        "\n",
        "'''\n",
        "Get a random input output pair to be used for training \n",
        "Refer the tutorial in the spec\n",
        "'''\n",
        "def random_training_pair(X, y):\n",
        "    rand_index = random.randint(0, len(X) - 1)\n",
        "    rand_name = X[rand_index]\n",
        "    rand_gen = y[rand_index]\n",
        "    category_tensor = torch.tensor([rand_gen], dtype=torch.long)\n",
        "    name_tensor = name_to_tensor(rand_name)\n",
        "    return rand_gen, rand_name, category_tensor, name_tensor\n",
        "\n",
        "'''\n",
        "Input: trained model, a list of words\n",
        "Output: a list of class labels as integers\n",
        "'''\n",
        "def predict(model, X):\n",
        "    hidden = model.init_hidden()\n",
        "    predictions = []\n",
        "    for name in X:\n",
        "      name_tensor = name_to_tensor(name)\n",
        "      hidden = model.init_hidden()\n",
        "      for i in range(name_tensor.size()[0]): \n",
        "        output, hidden = model(name_tensor[i], hidden)\n",
        "      predictions.append(output)\n",
        "    return [category_from_output(prediction)[1] for prediction in predictions]\n",
        "\n",
        "'''\n",
        "Input: trained model, a list of words, a list of class labels as integers\n",
        "Output: The accuracy of the given model on the given input X and target y\n",
        "'''\n",
        "def calculateAccuracy(model, X, y):\n",
        "    return (predict(model, X) == y).sum() / X.shape[0]\n",
        "\n",
        "'''\n",
        "Train the model for one epoch/one training word.\n",
        "Input: X and y are lists of words as strings and classes as integers respectively\n",
        "'''\n",
        "def trainOneEpoch(model, criterion, optimizer, X, y):\n",
        "    # TODO: to implement\n",
        "\n",
        "'''\n",
        "Use this to train and save your classification model. \n",
        "Save your model with the filename \"model_classify\"\n",
        "'''\n",
        "def run():\n",
        "    num_epoches = 20000\n",
        "    X, y = readData(train)\n",
        "    model = CharRNNClassify()\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    acc_epoches = []\n",
        "    trn_epoches = []\n",
        "\n",
        "    for epoch in range(num_epoches):\n",
        "      #TODO: to implement this method\n",
        "      \n",
        "\n",
        "    X_test, y_test = readData(test)\n",
        "    accuracy_overall = calculateAccuracy(model, X_test, y_test)\n",
        "    print(accuracy_overall)\n",
        "\n",
        "    return acc_epoches, trn_epoches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln-k2bImF4gy"
      },
      "source": [
        "Record the resulting accuracies every 500 epoches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NumLRV62lS46"
      },
      "source": [
        "result = run()\n",
        "# print(result[0])\n",
        "# print(result[1])\n",
        "# print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPSZPqri7NcB"
      },
      "source": [
        "###1.1.3 Accuracy Plots:\n",
        "Playground: please record your accuracies on the training set (take the first 900 entries) and the dev set (which is 3000 entries) for each 500'th epoch. Plot the resulting training / dev accuracy graphs. Hint: save these accuracies during the run. (5')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT6jWPVc7NJb"
      },
      "source": [
        "# TODO: plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcwbYFtm8tsz"
      },
      "source": [
        "###1.1.4 Testing Set Accuracy (5')\n",
        "Submit your model's performance on the testing set. You can do this within the \"run\" function. Only a number is needed. To get full mark, we expect a number greater than 0.68."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiePGf4RNgeJ"
      },
      "source": [
        "## 1.2 GPT-2 Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQBNkSyORxZN"
      },
      "source": [
        "### 1.2.1 Model Setup and Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBOUm2j3TQ0b"
      },
      "source": [
        "Set up the GPT-2 model by transformers library and generate the text given `input_text` and `input_text2` respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM30NEPJPGiE"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn45qK8EOH-u"
      },
      "source": [
        "from transformers import pipeline, set_seed\n",
        "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWjbrBspW987"
      },
      "source": [
        "# Initialize tokenizer and model from pretrained GPT-2 model\n",
        "## TODO: set up the GPT2Tokenizer of 'gpt2'\n",
        "## TODO: set up the GPT2LMHeadModel of 'gpt2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHmJvxGhOHpm"
      },
      "source": [
        "input_text = \"We love CIS 520 Machine Learning in University of Pennsylvania\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqcK488PdR1d"
      },
      "source": [
        "# TODO: Get the tensor values of each tokenized word of the input text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpf1bWLmN0kU"
      },
      "source": [
        "def generate_text(input_text):\n",
        "    '''Generate the text'''\n",
        "    ## TODO: complete the code snippets below\n",
        "    input_ids = ## Get the input indices of input_text by tokenizer\n",
        "    output = ## Get the output indices by model, set max_length as 50 and temperature as 1\n",
        "    output_text = ## Get the output text by decoding the output indices via tokenizer\n",
        "    print(output_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUFI_RK9OW-z"
      },
      "source": [
        "generate_text(input_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8viQwkqwcIK"
      },
      "source": [
        "input_text2 = \"We take CIS 520 Machine Learning in University of Pennsylvania\"\n",
        "generate_text(input_text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yilKC0iSUpb"
      },
      "source": [
        "### 1.2.2 The Impact of Letter Case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZUopkmiTE1X"
      },
      "source": [
        "Get the tensor values and generated texts of \"Machine Learning\" and \"Machine learning\". Are they different? Are the token and model case-sensitive? Do you think the output make sense if they're different and why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6ncVn2wOswH"
      },
      "source": [
        "# TODO: Get the tensor values of 'Machine Learning' and 'Machine learning'. \n",
        "tokens_ML =\n",
        "tokens_Ml =\n",
        "print(tokens_ML, tokens_Ml)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h75zM_q9rgm7"
      },
      "source": [
        "generate_text(\"Machine Learning\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3sg6U_mrpNX"
      },
      "source": [
        "generate_text(\"Machine learning\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84XUSFcC0lwg"
      },
      "source": [
        "### 1.2.3 Bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUs333nVSyo8"
      },
      "source": [
        "Run the cells below and explain the bias of gender, race, etc. that you can notice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL00qOqR0viT"
      },
      "source": [
        "from transformers import pipeline, set_seed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUW6mZ3M0nOj"
      },
      "source": [
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKeuzZI71UYu"
      },
      "source": [
        "generator(\"The White man worked as a\", max_length=20, num_return_sequences=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsGaI1IL1WCe"
      },
      "source": [
        "generator(\"The Black man worked as a\", max_length=20, num_return_sequences=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWNF6RnI0-yi"
      },
      "source": [
        "generator(\"The Asian man worked as a\", max_length=20, num_return_sequences=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-L1iZ4n1OvG"
      },
      "source": [
        "generator(\"The Asian woman worked as a\", max_length=20, num_return_sequences=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfkUe7GI1dbb"
      },
      "source": [
        "generator(\"The boy wants to be a\", max_length=20, num_return_sequences=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWyh6-2w1g7e"
      },
      "source": [
        "generator(\"The girl wants to be a\", max_length=20, num_return_sequences=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms6m-c7IZP41"
      },
      "source": [
        "# Question 2. AutoML\n",
        "In this part, we will do some hands-on practices for **GridSearchCV** and **AutoML**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aswnIJOiZV2Z"
      },
      "source": [
        "%pip install auto-sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npMFwm1RZWfk"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.base import BaseEstimator\n",
        "import autosklearn.classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqroEJ-gZgOd"
      },
      "source": [
        "# Loading the Dataset\n",
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "\n",
        "X = digits['data']\n",
        "y = digits['target']\n",
        "\n",
        "\n",
        "p = np.random.permutation(len(X))\n",
        "X, y = X[p], y[p]\n",
        "\n",
        "X_train, y_train = X[:1500], y[:1500]\n",
        "X_test, y_test = X[1500:], y[1500:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc3jCpkOZsyS"
      },
      "source": [
        "##2.1 Model and Hyperparameter Searching with GridSearchCV\n",
        "\n",
        "GridSearchCV provides a exhaustive search over specified parameter values for an estimator.\n",
        "\n",
        "The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.\n",
        "\n",
        "For more information on GridSearchCV, please refer to this [document](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). \n",
        "\n",
        "We have provided some helper functions and example code as follows. You need to modify the parameter list by adding AdaBoost, support vector machine, and random forest classifier with appropriate set of hyperparameyers and ranges to search. Compare those models' performance and report the results in the write-up.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYS_Z_6rZqGf"
      },
      "source": [
        "class helper(BaseEstimator):\n",
        "\n",
        "    def __init__(self, estimator = None):\n",
        "      self.estimator = estimator\n",
        "      \n",
        "    def fit(self, X, y=None):\n",
        "      self.estimator.fit(X, y)\n",
        "      return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "      return self.estimator.predict(X)\n",
        "    \n",
        "    def score(self, X, y):\n",
        "      return self.estimator.score(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN-AQ7erZv32"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    ('trans', PCA()),\n",
        "    ('clf', helper()),\n",
        "])\n",
        "parameters = [\n",
        "    {\n",
        "        'clf__estimator': [MLPClassifier(max_iter = 2000)], \n",
        "        'clf__estimator__hidden_layer_sizes':[(3), (10), (10,10,10)],\n",
        "        'trans__n_components': (2, 8, 32)\n",
        "    },\n",
        "    ## TODO: add AdaBoost, support vector machine, and random forest classifier with appropriate set of hyperparameyers and ranges to search\n",
        "\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWHutAX0Zyaz"
      },
      "source": [
        "GS_object = GridSearchCV(pipeline, parameters, return_train_score=False)\n",
        "GS_object.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bC9RIg6cmwq"
      },
      "source": [
        "## TODO: compare models' performance and report the results\n",
        "## print the best model with best set of searched params and the test accuracy; also report it in the write-up\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHkv2CgiaAcP"
      },
      "source": [
        "## 2.2 AutoML\n",
        "\n",
        "Auto-sklearn frees a machine learning user from algorithm selection and hyperparameter tuning. It leverages recent advantages in Bayesian optimization, meta-learning and ensemble construction. \n",
        "\n",
        "There are lots configuration options provided as arguments to the AutoSklearn. Please refer to this [document](https://automl.github.io/auto-sklearn/master/).\n",
        "\n",
        "The optimization process will run for as long as you allow, measure in minutes. By default, it will run for one hour. For timely manner, we will restrict 'time_left_for_this_task' to be 300 seconds. You are encouraged to explore other configuration options that will yield good results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmikKn3RaA2h"
      },
      "source": [
        "## TODO: Train AutoML on the dataset with 300 seconds \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqPgQErAgQf8"
      },
      "source": [
        "## TODO: Print the final ensemble constructed by auto-sklearn and take a look\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXsmld3JnEtW"
      },
      "source": [
        "## TODO: Report the 3 classifier choices with the largest ensemble_weight in this ensumble.\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
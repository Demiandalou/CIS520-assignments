{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw5_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAqXcDzKwj4D"
      },
      "source": [
        "# Homework 5: Coding\n",
        "\n",
        "**Due Monday October 11th, 11:59pm.**\n",
        "\n",
        "\n",
        "**In order to avoid module version issues, please complete this assignment on Colab.**\n",
        "\n",
        "**Submit hw5.py file to Gradescope (note there is no autograder for this assignment).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqy1LtZCIQuG"
      },
      "source": [
        "\"\"\"\n",
        "Import libraries that you might require\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "from sklearn.metrics import accuracy_score\n",
        "import sklearn.model_selection as ms\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qmuIim2IQuK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0a5bbf3-ea38-4ec6-dac2-8801eab5a0b7"
      },
      "source": [
        "\"\"\"\n",
        "Load data (MNIST digits dataset).\n",
        "\n",
        "Note that we will skip the validation phase for\n",
        "this exercise as by now you are pretty familiar with the typical Machine Learning\n",
        "pipeline.\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "print(digits.data.shape)\n",
        "\n",
        "X = digits['data']\n",
        "y = digits['target']\n",
        "\n",
        "np.random.seed(100)\n",
        "p = np.random.permutation(len(X))\n",
        "X, y = X[p], y[p]\n",
        "\n",
        "X_train, y_train = X[:1500], y[:1500]\n",
        "X_test, y_test = X[1500:], y[1500:]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1797, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmURULzoXCvq"
      },
      "source": [
        "# Question 2: Performance Comparisons for three ML algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BulL-miy2W1a"
      },
      "source": [
        "## 2.0 Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En_czqit2Wip"
      },
      "source": [
        "def train(models, X_train, y_train, X_test, y_test):\n",
        "  \"\"\"\n",
        "  Trains several models and returns the test accuracy for each of them\n",
        "  Args:\n",
        "      models: list of model objects\n",
        "  Returns:\n",
        "      score (float): list of accuracies of the different fitted models on test set\n",
        "  \"\"\"\n",
        "\n",
        "  # To complete: train and test each model in a for lop\n",
        "\n",
        "  accuracies = []\n",
        "\n",
        "\n",
        "  return accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_EG5SVkxrzs"
      },
      "source": [
        "## 2.1 Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KC2aNmoXCvr"
      },
      "source": [
        "def modelRF(n_estimators):\n",
        "  \"\"\"\n",
        "  Creates model objects for the Random Forest Classifier.\n",
        "  See the documentation in sklearn here:\n",
        "  https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "  Args:\n",
        "    n_estimators: list of hyper parameters\n",
        "  return:\n",
        "    list of classifiers\n",
        "  \"\"\"\n",
        "  \n",
        "  list_n_estimators = n_estimators\n",
        "  random_state = 20 # Do not change this random_state\n",
        "\n",
        "\n",
        "  objs_RFC = []\n",
        "  \n",
        "  # To complete: Create a list of objects for the classifier for each of the above \"n_estimators\"\n",
        "\n",
        "  return objs_RFC\n",
        " \n",
        "\n",
        "# To complete: call the above function to train and test the Random Forest Classifier\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJrNWsSgz6yh"
      },
      "source": [
        "## 2.2 Kernel SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLg9s5T4pAus"
      },
      "source": [
        "def modelKSVM():\n",
        "  \"\"\"\n",
        "  Creates model objects for the Kernel SVM.\n",
        "  See the documentation in sklearn here:\n",
        "  https://scikit-learn.org/stable/modules/svm.html\n",
        "  \"\"\"\n",
        "  \n",
        "  list_kernel_type = ['linear', 'poly', 'rbf']\n",
        "  random_state = 20 # Do not change this random_state\n",
        "\n",
        "  objs_KSVM = []\n",
        "  \n",
        "  # To complete: Create a list of objects for the classifier for each of the above \"kernel\" types\n",
        "\n",
        "  return objs_KSVM\n",
        "\n",
        "# To complete: Call the above function to train and test the Kernel SVM\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vSc15PpxyS2"
      },
      "source": [
        "## 2.3 Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejmbeisHpBdO"
      },
      "source": [
        "def modelMLP():\n",
        "  \"\"\"\n",
        "  Creates model objects for the Multi Layered Perceptron.\n",
        "  See the documentation in sklearn here:\n",
        "  https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "  \"\"\"\n",
        "  \n",
        "  \n",
        "  layerSizes = [(3), (10), (10,10,10), (20,50,20)]\n",
        "  random_state = 20 # Do not change this random_state\n",
        "  max_iter = 2000 # fixed max_iter\n",
        "  \n",
        "  objs_MLP = []\n",
        "\n",
        "  # To complete: Create a list of objects for the classifier for each of the above \"layerSizes\"\n",
        "\n",
        "  return objs_MLP\n",
        "\n",
        "# To complete: Call the above function to train and test the Multi Layer Perceptron\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82t0UEM2fYOM"
      },
      "source": [
        "## 2.4 AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mceSBJ_fXU0"
      },
      "source": [
        "def modelAdaBoost():\n",
        "  \"\"\"\n",
        "  Creates model objects for the AdaBoost.\n",
        "  See the documentation in sklearn here:\n",
        "  https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
        "  \"\"\"\n",
        "  num_estimators = [1,5,10,50,100,150]\n",
        "  learning_rate = 0.1\n",
        "  max_depth = 3\n",
        "  random_state = 20 # Do not change this random_state\n",
        "  # To complete: Create a list of objects for the classifier for each of combination of above num_estimators and learning_rate\n",
        "  obj_boost = []\n",
        "\n",
        "  return obj_boost\n",
        "\n",
        "# To complete: Call the above function to train and test the AdaBoost Classifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWS884Qt13Bd"
      },
      "source": [
        "!rm -r -f ./logs\n",
        "\n",
        "LOG_DIR = './logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvbnO1JX13BO"
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0lafkzw13Ak",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d187507e-ab67-4d81-855a-d25870167aef"
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipIykcBHt1dk"
      },
      "source": [
        "# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.misc \n",
        "try:\n",
        "    from StringIO import StringIO  # Python 2.7\n",
        "except ImportError:\n",
        "    from io import BytesIO         # Python 3.x\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    \n",
        "    def __init__(self, log_dir):\n",
        "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
        "        self.writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "    def scalar_summary(self, tag, value, step):\n",
        "        \"\"\"Log a scalar variable.\"\"\"\n",
        "        with self.writer.as_default():\n",
        "            tf.summary.scalar(name=tag, data=value, step=step)\n",
        "        self.writer.flush()\n",
        "\n",
        "    def image_summary(self, tag, images, step):\n",
        "        \"\"\"Log a list of images.\"\"\"\n",
        "\n",
        "        for i, img in enumerate(images):\n",
        "            # Write the image to a string\n",
        "            try:\n",
        "                s = StringIO()\n",
        "            except:\n",
        "                s = BytesIO()\n",
        "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
        "\n",
        "            # Create an Image object as a Summary value\n",
        "            with self.writer.as_default():\n",
        "                tf.summary.image(name='%s/%d' % (tag, i), data=s.getvalue(), step=step)\n",
        "\n",
        "        # Create and write Summary\n",
        "        self.writer.flush()\n",
        "        \n",
        "    def histo_summary(self, tag, values, step, bins=1000):\n",
        "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
        "\n",
        "        # Create a histogram using numpy\n",
        "        counts, bin_edges = np.histogram(values, bins=bins)\n",
        "\n",
        "        # Fill the fields of the histogram proto\n",
        "        hist = tf.HistogramProto()\n",
        "        hist.min = float(np.min(values))\n",
        "        hist.max = float(np.max(values))\n",
        "        hist.num = int(np.prod(values.shape))\n",
        "        hist.sum = float(np.sum(values))\n",
        "        hist.sum_squares = float(np.sum(values**2))\n",
        "\n",
        "        # Drop the start of the first bin\n",
        "        bin_edges = bin_edges[1:]\n",
        "\n",
        "        # Add bin edges and counts\n",
        "        for edge in bin_edges:\n",
        "            hist.bucket_limit.append(edge)\n",
        "        for c in counts:\n",
        "            hist.bucket.append(c)\n",
        "\n",
        "        # Create and write Summary\n",
        "        with self.writer.as_default():\n",
        "            tf.summary.histogram(name=tag, data=hist, step=step)\n",
        "        self.writer.flush()\n",
        "logger = Logger('./logs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIkuagd9_X1v"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK7aviwP_X1x"
      },
      "source": [
        "# Define a transform to normalize the data\n",
        "\n",
        "#TODO : Set the value of mean and the standard deviation to \n",
        "#       normalize the image from range [0,1] to the range [-1, 1]\n",
        "\n",
        "\n",
        "#Begin Your Code\n",
        "\n",
        "mean = \n",
        "std = \n",
        "\n",
        "#End Your Code\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((mean,), (std,))\n",
        "                                ])\n",
        "\n",
        "\n",
        "#TODO : Select suitable value of batch_sizes.\n",
        "\n",
        "#Begin Your Code\n",
        "\n",
        "train_batch_size = \n",
        "test_batch_size = \n",
        "\n",
        "#End Your Code\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# Classes\n",
        "classes = {       0 :'T-shirt/top',\n",
        "                  1 :'Trouser',\n",
        "                  2 :'Pullover',\n",
        "                  3 :'Dress',\n",
        "                  4 :'Coat',\n",
        "                  5 :'Sandal',\n",
        "                  6 :'Shirt',\n",
        "                  7 :'Sneaker',\n",
        "                  8 :'Bag',\n",
        "                  9 :'Ankle boot'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX5Irrw-_X1z"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import numpy as np\n",
        "\n",
        "# Functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    \n",
        "    figure(num=None, figsize=(8, 6), dpi=150, edgecolor='k')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWmqB-1b_X11"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        #TODO : Design your network, you are allowed to explore your own architecture\n",
        "        #       But you should achieve a better overall accuracy than the baseline network.\n",
        "        #       Also, if you do design your own network, include an explanation \n",
        "        #       for your choice of network and how it may be better than the \n",
        "        #       baseline network in your latex.\n",
        "        \n",
        "        #Begin Your Code\n",
        "\n",
        "\n",
        "        #End Your Code\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      #TODO : Implement the forward function that applies the layers you have created to the input\n",
        "\n",
        "      #Begin Your Code\n",
        "\n",
        "\n",
        "      #End Your Code\n",
        "\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5-Qn1ef_X13"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "#TODO : Use appropriate loss criterion and optimizer \n",
        "\n",
        "#Begin Your Code\n",
        "\n",
        "criterion = \n",
        "optimizer = \n",
        "\n",
        "#End Your Code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7y1qZI1wyvv"
      },
      "source": [
        "overall_step = 0\n",
        "\n",
        "#TODO : Select appropriate number of epochs\n",
        "\n",
        "#Begin Your Code\n",
        "\n",
        "epochs =\n",
        "\n",
        "#End Your Code\n",
        "\n",
        "\n",
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "    running_loss = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        #TODO : Make predictions, calculate accuracy and update your weights once\n",
        "\n",
        "        #Begin Your Code\n",
        "\n",
        "        # zero the parameter gradients\n",
        "\n",
        "        # forward + backward + optimize\n",
        "\n",
        "        #End Your Code\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print('Epoch: %d, Batch: %5d, loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "            #Any thing that is added to the \"info\" gets plotted in tensorboard\n",
        "            #TODO : Add the plots in Tensorboard to the report and explain what is happening\n",
        "            info = {'loss' : loss.item(), 'accuracy': accuracy.item()}\n",
        "            for tag, value in info.items():\n",
        "                logger.scalar_summary(tag, value, overall_step+1)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTyWZj2R_X2F"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "#TODO : Report this accuracy in your report.\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO9h7Zbd_X2H"
      },
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnvvXuO2z-kt"
      },
      "source": [
        "\n",
        "# Turning it in\n",
        "\n",
        "**This notebook will not be autograded, so no need to comment out code outside of functions.**\n",
        "\n",
        "1. Download this notebook as a `hw5.py` file with the functions implemented and the sandbox code commented out\n",
        "  - go to \"File -> Download .py\"\n",
        "  \n",
        "2. Submit `hw4.py` file to Gradescope (you can do this as many times as you'd like before the deadline)"
      ]
    }
  ]
}
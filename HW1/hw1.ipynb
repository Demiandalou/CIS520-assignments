{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('base': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "interpreter": {
      "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMMASUUdm4E7"
      },
      "source": [
        "# Homework 1 (Coding)\n",
        "\n",
        "**Due Monday 21st September, 11:59pm**\n",
        "\n",
        "**Submit the hw1.ipynb file to gradescope.**\n",
        "\n",
        "To download the .ipynb version go to File->Download as .ipynb\n",
        "\n",
        "\n",
        "Retain the outputs generated by the cells when you upload the notebook.\n",
        "\n",
        "\n",
        "If you are working in pairs make sure to add your team memberâ€™s name on Gradescope when submitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqy1LtZCIQuG"
      },
      "source": [
        "\"\"\"\n",
        "Import libraries that you might require\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAfePG6PyjNp"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2lGPqwTymkU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1e34f353-3420-4e59-8046-cdbd7f3876e0"
      },
      "source": [
        "url=\"https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/diabetes.csv\"\n",
        "response = requests.get(url).content\n",
        "diabetes = pd.read_csv(io.StringIO(response.decode('utf-8')))\n",
        "diabetes.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyGqB2h3XMp9"
      },
      "source": [
        "# print(set(diabetes['Outcome']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGU3nr9PzMq3"
      },
      "source": [
        "# Split the data into training, validation and testing sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = diabetes.to_numpy()\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "\n",
        "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, stratify=y, test_size = 0.25, random_state=66)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size = 0.4, random_state=66)\n",
        "# 75% train, 9% val, 6% test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMQDN_YzIQuK"
      },
      "source": [
        "# Question 4: KNN Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgH67-bV4AJ9"
      },
      "source": [
        "We will implement the KNN algorithm for the diabetes dataset. Refer to the pdf and the following functions for the instructions. Complete all the functions as indicated below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qmuIim2IQuK"
      },
      "source": [
        "\"\"\"\n",
        "Task 1: Classification\n",
        "\n",
        "Please implement KNN for K: 3, 5, and 7 \n",
        "with the following norms:\n",
        "L1\n",
        "L2\n",
        "L-inf\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def distanceFunc(metric_type, vec1, vec2):\n",
        "    \"\"\"\n",
        "    Computes the distance between two d-dim vectors\n",
        "    Args:\n",
        "        metric_type: String\n",
        "        vec1 (numpy vector): Vector\n",
        "        vec2 (numpy vector): Vector\n",
        "    Returns:\n",
        "        distance (float): distance between the two vectors\n",
        "    \"\"\"\n",
        "\n",
        "    diff = vec1 - vec2\n",
        "    if metric_type == \"L1\":\n",
        "        #complete\n",
        "        distance = np.sum([abs(i) for i in diff])\n",
        "\n",
        "    if metric_type == \"L2\":\n",
        "        #complete\n",
        "        # distance = np.linalg.norm(diff,ord=2)\n",
        "        distance = math.sqrt(sum([i**2 for i in diff]))\n",
        "        \n",
        "    if metric_type == \"L-inf\":\n",
        "        #complete\n",
        "        # distance = np.linalg.norm(diff,ord=np.inf)\n",
        "        distance = max([abs(i) for i in diff])\n",
        "        \n",
        "    return distance\n",
        "\n",
        "\n",
        "def computeDistancesNeighbors(K, metric_type, X_train, y_train, sample):\n",
        "    \"\"\"\n",
        "    Compute the distances between every datapoint in the train_data and the \n",
        "    given sample. Then, find the k-nearest neighbors.\n",
        "    Return a numpy array of the label of the k-nearest neighbors.\n",
        "    \n",
        "    Args:\n",
        "        K (int): K-value\n",
        "        metric_type (String): metric type, \"L1\", \"L2\",\"L-inf\"\n",
        "        X_train (numpy array): Training data\n",
        "        y_train : Training labels\n",
        "        sample (numpy vector): Sample whose distance is to computed with every entry in the dataset\n",
        "        \n",
        "    Returns:\n",
        "        neighbors (list): K-nearest neighbors' labels\n",
        "    \"\"\"\n",
        "    dist=[]\n",
        "    for i in range(len(X_train)):\n",
        "      dist.append(distanceFunc(metric_type, sample, X_train[i]))\n",
        "    # dist_dict = {dist[i]:i for i in range(len(dist))}\n",
        "    # sorted_dict = sorted(dist_dict.items(), key = lambda kv:(kv[0]))\n",
        "    # labelidx = [d[1] for d in sorted_dict[:K]]\n",
        "    labelidx = np.argpartition(dist,K)[:K]\n",
        "    neighbors = y_train[labelidx]\n",
        "    return neighbors\n",
        "\n",
        "# computeDistancesNeighbors(3, 'L1', X_train, y_train, X_train[0])\n",
        "def Majority(neighbors):\n",
        "    \"\"\"\n",
        "    Performs majority voting and returns the predicted value for the test sample\n",
        "    Args:\n",
        "        neighbors (list): K-nearest neighbors' labels\n",
        "    Returns:\n",
        "        predicted_value (int or float): predicted label for the given sample\n",
        "    \"\"\"\n",
        "    \n",
        "    # Performs majority voting\n",
        "    # Complete this function\n",
        "    if sum(neighbors)>len(neighbors)//2: # if number of label 1 is more than half\n",
        "        predicted_value = 1\n",
        "    else:\n",
        "        predicted_value = 0\n",
        "    \n",
        "    return predicted_value\n",
        "\n",
        "\n",
        "def KNN(K, metric_type, X_train, y_train, X_val):\n",
        "    \"\"\"\n",
        "    Returns the predicted values for the entire validation or test set\n",
        "    Args:\n",
        "        K (int): K-value\n",
        "        metric_type (String): metric type\n",
        "        X_train (numpy array): Training data\n",
        "        y_train : Training labels\n",
        "        X_val or X_test (numpy array): Validation or test data\n",
        "    Returns:\n",
        "        predicted_values (list): output for every entry in validation/test dataset \n",
        "    \"\"\"\n",
        "    \n",
        "    # Complete this function\n",
        "    # Loop through the val_data or the test_data (as required)\n",
        "    # and compute the output for every entry in that dataset  \n",
        "    # You will also call the function \"Majority\" here\n",
        "    predictions = []\n",
        "    for sample in X_val: # compute neighbor labels for each data point in val set\n",
        "        neighbors = computeDistancesNeighbors(K, metric_type, X_train, y_train, sample)\n",
        "        predicted_value = Majority(neighbors)\n",
        "        predictions.append(predicted_value)\n",
        "        \n",
        "    return predictions\n",
        "\n",
        "\n",
        "def evaluation(predicted_values, actual_values):\n",
        "    \"\"\"\n",
        "    Computes the accuracy of the given datapoints.\n",
        "    \n",
        "    Args:\n",
        "        predicted_values: vector\n",
        "        actual_values: numpy vector\n",
        "    \n",
        "    Returns:\n",
        "        accuracy (float): accuracy\n",
        "    \"\"\"\n",
        "    \n",
        "    return accuracy_score(predicted_values, actual_values)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Calls the above functions in order to implement the KNN algorithm.\n",
        "    \n",
        "    Test over the following range K = 3,5,7 and all three metrics. \n",
        "    In total you will have nine combinations to try.\n",
        "    \n",
        "    PRINTS out the accuracies for the nine combinations on the validation set,\n",
        "    and the accuracy on the test set for the selected K value and appropriate norm.\n",
        "    \"\"\"\n",
        "    \n",
        "    ## Complete this function\n",
        "    \n",
        "    K = [3,5,7]\n",
        "    norm = [\"L1\", \"L2\", \"L-inf\"]\n",
        "    \n",
        "    print(\"<<<<VALIDATION DATA PREDICTIONS>>>>\")\n",
        "    for k in K: # iterate through k-value and norms\n",
        "        for metric_type in norm:\n",
        "            pred_val = KNN(k, metric_type, X_train, y_train, X_val)\n",
        "            acc = evaluation(pred_val, y_val)\n",
        "            print('K=%d,\\t norm=%s,\\t accuracy= %.4f'%(k,metric_type,acc))\n",
        "    ## Complete\n",
        "\n",
        "    print(\"<<<<TEST DATA PREDICTIONS>>>>\")\n",
        "    selected_k = 7\n",
        "    appr_norm = \"L2\"\n",
        "    # given selected k and norm, calculate the test set accuracy\n",
        "    pred_val = KNN(selected_k, appr_norm, X_train, y_train, X_test)\n",
        "    acc = evaluation(pred_val, y_test)\n",
        "    print('K=%d,\\t norm=%s,\\t accuracy= %.4f'%(k,appr_norm,acc))\n",
        "    ## Complete\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW4D4AbBjhK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91486eca-a392-4e62-a2b7-daaaac0d2067"
      },
      "source": [
        "# Finally, call the main function\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<<VALIDATION DATA PREDICTIONS>>>>\n",
            "K=3,\t norm=L1,\t accuracy= 0.7217\n",
            "K=3,\t norm=L2,\t accuracy= 0.6957\n",
            "K=3,\t norm=L-inf,\t accuracy= 0.7304\n",
            "K=5,\t norm=L1,\t accuracy= 0.7565\n",
            "K=5,\t norm=L2,\t accuracy= 0.7652\n",
            "K=5,\t norm=L-inf,\t accuracy= 0.7217\n",
            "K=7,\t norm=L1,\t accuracy= 0.7304\n",
            "K=7,\t norm=L2,\t accuracy= 0.7739\n",
            "K=7,\t norm=L-inf,\t accuracy= 0.7217\n",
            "<<<<TEST DATA PREDICTIONS>>>>\n",
            "K=7,\t norm=L2,\t accuracy= 0.7143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crlfaFAHmxLM",
        "outputId": "64e3a186-ee00-4eb2-9ab3-1ba3a54df75b"
      },
      "source": [
        "computeDistancesNeighbors(2, \"L1\", np.array([[0,0],[0,1],[1,0]]), np.array([1,2,3]), np.array([1,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmURULzoXCvq"
      },
      "source": [
        "# Question 5: Decision Tree Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-_QAM3jXCvq"
      },
      "source": [
        "### Helper functions\n",
        "The block below contains helper functions for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KC2aNmoXCvr"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Below are a list of helper functions to use to help you on this task\n",
        "\n",
        "def train_decision_tree(X, y, depth=None, leaf_count=None):\n",
        "  \"\"\"\n",
        "  Trains a decision tree classifier on the given X, y data with the specified \n",
        "  tree depth d and max leaf node count max_leaf_num.\n",
        "  \n",
        "  Args:\n",
        "    X ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by\n",
        "                          p (number of features) matrix\n",
        "    y ((n,) np.ndarray): The input ys, which are in an n length array\n",
        "    depth (int): The maximum depth of the tree. A value of None means no restrictions\n",
        "             on the depth of the tree.\n",
        "    leaf_count (int): The maximum leaf count of the tree's leaf nodes. A value of None means \n",
        "    no restrictions on the leaf count of the tree.\n",
        "  \n",
        "  Returns:\n",
        "    clf(DecisionTreeClassifier): the trained decision tree classifier\n",
        "  \"\"\"\n",
        "  clf = DecisionTreeClassifier(max_depth=depth, max_leaf_nodes=leaf_count, criterion=\"entropy\", random_state=1)\n",
        "  clf.fit(X,y)\n",
        "  return clf\n",
        "\n",
        "def predict(clf, X_test):\n",
        "  \"\"\"\n",
        "  Uses a trained decision tree classifier to predict on a given test set.\n",
        "  \n",
        "  Args:\n",
        "    clf (DecisionTreeClassifier): Trained Decision Tree Classifer\n",
        "    X_test ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by\n",
        "                               p (number of features) matrix\n",
        "  \n",
        "  Returns:\n",
        "    y_pred ((n,) np.ndarray): The output predictions, which are in an n length array\n",
        "  \"\"\"\n",
        "  y_pred = clf.predict(X_test)\n",
        "  return y_pred\n",
        "\n",
        "def evaluate(predicted_values, actual_values):\n",
        "    \"\"\"\n",
        "    Computes the accuracy of the given datapoints.\n",
        "    \n",
        "    Args:\n",
        "        predicted_values: numpy array\n",
        "        actual_values: numpy array\n",
        "    \n",
        "    Returns:\n",
        "        a floating point number representing the accuracy\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    return accuracy_score(predicted_values, actual_values)\n",
        "  \n",
        "def plot_line_graph(x_vals, y_vals_1, y_vals_2, y_vals_1_label, y_vals_2_label, x_axis_label, y_axis_label, title):\n",
        "  \"\"\"\n",
        "  Plots a line graph of two lines of different values with common x-values\n",
        "  \n",
        "  Args:\n",
        "    x_vals ((j,) list): Values to be displayed on horizontal axis, where j is number of values\n",
        "    y_vals_1 ((j,) list): First set of values to be graphed on a line in respect to x_vals, where j is number of values\n",
        "    y_vals_2 ((j,) list): Second set of values to be graphed on a line in respect to x_vals, where j is number of values\n",
        "    y_vals_1_label (string): Label for first set of y values\n",
        "    y_vals_2_label (string): Label for second set of y values\n",
        "    x_axis_label (string): Label for x axis\n",
        "    y_axis_label (string): Label for y axis\n",
        "    title (string): Plot title\n",
        "  \"\"\"\n",
        "  \n",
        "  plt.plot(x_vals, y_vals_1, color='g', label=y_vals_1_label)\n",
        "  plt.plot(x_vals, y_vals_2, color='orange', label=y_vals_2_label)\n",
        "  plt.xlabel(x_axis_label)\n",
        "  plt.ylabel(y_axis_label)\n",
        "  plt.title(title)\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP9j8K21XCvs"
      },
      "source": [
        "### Compare Accuracy for full classification dataset as well as smaller classification dataset\n",
        "We will be using the diabetes classification dataset. You are also given a smaller training dataset with the same data as the full dataset but with only half of the sample number. We will observe the performance changes when less data is available.\n",
        "\n",
        "To start, uncomment the code below and run to create small dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRothkZeXCvt"
      },
      "source": [
        "# We will also use the same diabetes classification dataset in Task 1.\n",
        "# Let's create a smaller version of the training dataset using only half of the data available\n",
        "\n",
        "train_sample_num_small = int(X_train.shape[0] / 2)\n",
        "X_train_small, y_train_small = X_train[:train_sample_num_small], y_train[:train_sample_num_small]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEsSQwRMXCvv"
      },
      "source": [
        "### Base Metrics on Full and Partial Data\n",
        "To start, you will be comparing the training and testing accuracies of both datasets given a vanilla decision tree.\n",
        "\n",
        "Note: Make sure to create two separate classifiers for each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfRXN92DXCvv"
      },
      "source": [
        "def base_metrics(X_train, y_train, X_train_small, y_train_small, X_val, y_val, X_test, y_test):\n",
        "  \"\"\"\n",
        "  Create a decision tree classifer on the full dataset and the partial dataset (only half of n).\n",
        "    \n",
        "  Args: (Note that n is not the same among train and test sets, but merely refers to sample size)\n",
        "    X_train ((n,p) np.ndarray): Input feature matrix of full dataset for training/fitting\n",
        "    y_train ((n,) np.ndarray): Input label array of full dataset for training/fitting\n",
        "    X_train_small ((n,p) np.ndarray): Input feature matrix of partial/small dataset for training/fitting\n",
        "    y_train_small ((n,) np.ndarray): Input label array of partial/small dataset for training/fitting\n",
        "    X_val ((n,p) np.ndarray): Input feature matrix of full dataset for validation\n",
        "    y_val ((n,) np.ndarray): Input label array of full dataset for validation\n",
        "    X_test ((n,p) np.ndarray): Input feature matrix of full dataset for testing\n",
        "    y_test ((n,) np.ndarray): Input label array of full dataset for testing    \n",
        "\n",
        "  To observe:\n",
        "    train_acc_full_set (float): Training accuracy using a model trained on the full dataset\n",
        "    val_acc_full_set (float): Validation accuracy using a model trained on the full dataset\n",
        "    test_acc_full_set (float): Test accuracy using a model trained on the full dataset\n",
        "    train_acc_small_set (float): Training accuracy using a model trained on the small dataset\n",
        "    val_acc_small_set (float): Validation accuracy using a model trained on the small dataset\n",
        "    test_acc_small_set (float): Test accuracy using a model trained on the small dataset\n",
        "  \"\"\"\n",
        "  \n",
        "  # <---- Your code here ----->\n",
        "  # create decision tree for full dataset, and eval accuracies\n",
        "  clf_full = train_decision_tree(X_train, y_train, depth=None, leaf_count=None)\n",
        "  train_acc_full_set = evaluate(predict(clf_full, X_train), y_train)\n",
        "  val_acc_full_set = evaluate(predict(clf_full, X_val), y_val)\n",
        "  test_acc_full_set = evaluate(predict(clf_full, X_test), y_test)\n",
        "\n",
        "  # create decision tree for small dataset, and eval accuracies\n",
        "  clf_small = train_decision_tree(X_train_small, y_train_small, depth=None, leaf_count=None)\n",
        "  train_acc_small_set = evaluate(predict(clf_small, X_train_small), y_train_small)\n",
        "  val_acc_small_set = evaluate(predict(clf_small, X_val), y_val)\n",
        "  test_acc_small_set = evaluate(predict(clf_small, X_test), y_test)\n",
        "\n",
        "  # <---- Your code ends here ----->\n",
        "  \n",
        "  print(\"Train Accuracy on Full Dataset: \", train_acc_full_set)\n",
        "  print(\"Validation Accuracy on Full Dataset: \", val_acc_full_set)\n",
        "  print(\"Test Accuracy on Full Dataset: \", test_acc_full_set)\n",
        "  print(\"Train Accuracy on Small (Half) Dataset: \", train_acc_small_set)\n",
        "  print(\"Validation Accuracy on Small (Half) Dataset: \", val_acc_small_set)\n",
        "  print(\"Test Accuracy on Small (Half) Dataset: \", test_acc_small_set)\n",
        "  \n",
        "  return (train_acc_full_set, \n",
        "          val_acc_full_set, \n",
        "          test_acc_full_set,\n",
        "          train_acc_small_set, \n",
        "          val_acc_small_set, \n",
        "          test_acc_small_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTH2bUBY3SCP"
      },
      "source": [
        "Uncomment the code below and run the code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM9BV2i3XCvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efdb61d8-2a1c-432b-ff02-97b4515beeb3"
      },
      "source": [
        "base_metrics(X_train, y_train, X_train_small, y_train_small, X_val, y_val, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy on Full Dataset:  1.0\n",
            "Validation Accuracy on Full Dataset:  0.7043478260869566\n",
            "Test Accuracy on Full Dataset:  0.7532467532467533\n",
            "Train Accuracy on Small (Half) Dataset:  1.0\n",
            "Validation Accuracy on Small (Half) Dataset:  0.7130434782608696\n",
            "Test Accuracy on Small (Half) Dataset:  0.6753246753246753\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0,\n",
              " 0.7043478260869566,\n",
              " 0.7532467532467533,\n",
              " 1.0,\n",
              " 0.7130434782608696,\n",
              " 0.6753246753246753)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gUUoJSO3SCR"
      },
      "source": [
        "### Question 5.1 Report on LaTeX\n",
        "Answer the following questions on LaTeX in the respective section.\n",
        "1. Report the results of the accuracies on LateX \n",
        "2. Which dataset had a higher difference between training and test accuracy? Briefly explain why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xiF5-LvXCvz"
      },
      "source": [
        "### Improving Decision Tree for Smaller Dataset by Tuning Hyperparameters\n",
        "Classifiers often overfit on smaller datasets, so now, we will optimize hyperparameters on tree depth and max leaf count to improve the performance of our model. \n",
        "\n",
        "Fill out the helper functions below which will take an array of hyperparameter values for tree depth and an array of hyperparameter values for max leaf count. The helper function will return a training and validation accuracy scores for each pair of hyperparameter values. This is referred to as grid search for hyperparameter tuning.\n",
        "\n",
        "At the end, the function identifies the best value of the tree depth and tree node count hyperparameters for a dataset, as well as the final training and testing scores.\n",
        "\n",
        "Note: Use the highest validation score to choose the optimal hyperparameter combination. If there is a tie, use the lower hyperparameter value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1nPIPFmXCvz"
      },
      "source": [
        "def grid_search_depth_and_leaf_count(depth_search_space, leaf_count_search_space, X_train, y_train, X_val, y_val):\n",
        "  \"\"\"\n",
        "  Perform a decision tree hyperparameter grid search on tree depth and leaf count given training and validation data.\n",
        "    \n",
        "  Args:\n",
        "    depth_search_space ((d,) list): Tree depth values to search over, i.e. [1, 3, 6, 10, 30]\n",
        "    leaf_count_search_space ((l,) list): Max leaf count values to search over, i.e. [2, 3, 4, 5, 6]\n",
        "    X_train ((n, p) np.ndarray): The input feature matrix for training\n",
        "    y_train ((n, p) np.ndarray): The input ys for training\n",
        "    X_val ((n, p) np.ndarray): The input feature matrix that will be used to validate accuracy scores\n",
        "    y_val ((n, p) np.ndarray): The input ys that will be used to validate accuracy scores\n",
        "    \n",
        "  To return:\n",
        "    best_depth (int): The depth count in the hyperparameter combination with the largest validation score\n",
        "    best_leaf_count (int): The leaf count in the hyperparameter combination with the largest validation score\n",
        "  \"\"\"\n",
        "  \n",
        "  # <---- Your code here ----->\n",
        "  param_pair_acc = {}\n",
        "  for dep in depth_search_space:\n",
        "    for leaf in leaf_count_search_space:\n",
        "      clf = train_decision_tree(X_train, y_train, depth=dep, leaf_count=leaf)\n",
        "      acc = evaluate(predict(clf, X_val), y_val)\n",
        "      param_pair_acc[(dep,leaf)] = acc\n",
        "  param_pair_acc = sorted(param_pair_acc.items(), key = lambda kv: kv[1],reverse=True)\n",
        "  # print(param_pair_acc)\n",
        "  # param_pair_acc = [i[0] for i in param_pair_acc if i[1]==param_pair_acc[0][1]]\n",
        "  # param_pair_acc.sort()\n",
        "  best_depth = param_pair_acc[0][0][0]\n",
        "  best_leaf_count = param_pair_acc[0][0][1]\n",
        "  \n",
        "  # <---- Your code ends here ----->\n",
        "  print(\"Chosen Depth: \", best_depth)\n",
        "  print(\"Chosen Leaf: \", best_leaf_count)\n",
        "  \n",
        "  return best_depth, best_leaf_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvCBkKLJ3SCV"
      },
      "source": [
        "Uncomment and run the code below to and record the best depth and best leaf count hyperparameters on LaTeX in the respective section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyZZ2CHb3SCW",
        "outputId": "9ce1ba23-25a3-46d3-9d49-66054cb5b45c"
      },
      "source": [
        "# Search spaces for grid search to tune tree depth and leaf count hyperparameters\n",
        "depth_search_space = [1, 3, 6, 10, 30]\n",
        "leaf_count_search_space = [2, 3, 4, 5, 6]\n",
        "\n",
        "print(\"FULL DATASET\")\n",
        "grid_search_depth_and_leaf_count(depth_search_space, \n",
        "                                 leaf_count_search_space, \n",
        "                                 X_train, \n",
        "                                 y_train, \n",
        "                                 X_val, \n",
        "                                 y_val)\n",
        "\n",
        "print(\"\\nSMALL DATASET\")\n",
        "grid_search_depth_and_leaf_count(depth_search_space, \n",
        "                                 leaf_count_search_space, \n",
        "                                 X_train_small, \n",
        "                                 y_train_small, \n",
        "                                 X_val,\n",
        "                                 y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL DATASET\n",
            "Chosen Depth:  3\n",
            "Chosen Leaf:  4\n",
            "\n",
            "SMALL DATASET\n",
            "Chosen Depth:  1\n",
            "Chosen Leaf:  2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_oY8swF3SCX"
      },
      "source": [
        "### Question 5.2 Report on LaTeX\n",
        "Answer the following questions on LaTeX in the respective section.\n",
        "1. Report the chosen hyperparameters for both the complete set and the partial set.\n",
        "2. Did the small dataset have higher or lower chosen hyperparameter values than the full dataset? Briefly explain why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e85OLhHfXCv_"
      },
      "source": [
        "### Retrain Decision Tree and Plot Hyperparameter Search\n",
        "Now retrain your decision tree with the optimal hyperparameters. Report training, validation, and testing error for the small dataset.\n",
        "\n",
        "Also for the small dataset, create a graph plotting the training and validation scores for each leaf node hyperparameter value, holding the tree depth hyperparameter consistent at the chosen value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrWrji_iXCwA"
      },
      "source": [
        "def retrain_decision_tree(X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "  \n",
        "  \"\"\"\n",
        "  Perform a decision tree hyperparameter grid search given training and validation data and search values for\n",
        "  tree depth and leaf node count.\n",
        "    \n",
        "  Args: (Note that n is not the same among train and test sets, but merely refers to sample size)\n",
        "    X_train ((n,p) np.ndarray)\n",
        "    y_train ((n,) np.ndarray)\n",
        "    X_val ((n,p) np.ndarray)\n",
        "    y_val ((n,) np.ndarray)\n",
        "    X_test ((n,p) np.ndarray)\n",
        "    y_test ((n,) np.ndarray)\n",
        "\n",
        "  To return:\n",
        "    train_acc (float): Optimal Hyperparameters Train Accuracy\n",
        "    val_acc (float): Optimal Hyperparameters Train Accuracy\n",
        "    test_acc (float): Optimal Hyperparameters Train Accuracy\n",
        "    \n",
        "    leaf_count_train_scores (list): Report training scores for max leaf count search space\n",
        "    leaf_count_val_scores (list): Report validation scores for max leaf count search space\n",
        "  \"\"\"\n",
        "  # Select best hyperparameters\n",
        "  depth_search_space = [2, 4, 6, 8, 10, 16, 20]\n",
        "  leaf_count_search_space = [2, 3, 4, 5, 6, 7, 8, 9, 10] \n",
        "\n",
        "  chosen_depth, chosen_leaf_count = grid_search_depth_and_leaf_count(depth_search_space, \n",
        "                                                                     leaf_count_search_space, \n",
        "                                                                     X_train, \n",
        "                                                                     y_train, \n",
        "                                                                     X_val, \n",
        "                                                                     y_val)\n",
        "  \n",
        "  # <---- Your code here ----->   \n",
        "  # train decision tree with Optimal Hyperparameters\n",
        "  clf_full = train_decision_tree(X_train, y_train, depth=chosen_depth, leaf_count=chosen_leaf_count)\n",
        "  train_acc = evaluate(predict(clf_full, X_train), y_train)\n",
        "  val_acc = evaluate(predict(clf_full, X_val), y_val)\n",
        "  test_acc = evaluate(predict(clf_full, X_test), y_test)\n",
        "\n",
        "  leaf_count_train_scores, leaf_count_val_scores = [], []\n",
        "  for leaf in leaf_count_search_space:\n",
        "    # keep chosen depth, iterate through leaf counts\n",
        "    clf = train_decision_tree(X_train, y_train, depth=chosen_depth, leaf_count=leaf)\n",
        "    # record train score and val scores for different leaf counts\n",
        "    leaf_count_train_scores.append(evaluate(predict(clf, X_train), y_train))\n",
        "    leaf_count_val_scores.append(evaluate(predict(clf,X_val), y_val))\n",
        "  plot_line_graph(leaf_count_search_space, leaf_count_train_scores, leaf_count_val_scores, \n",
        "                y_vals_1_label='Training', y_vals_2_label='Validation', \n",
        "                x_axis_label='Leaf node', y_axis_label='Scores', title='Scores for each leaf node hyperparameter')\n",
        "  # plt.savefig('ques_tree_leaf_node.png',dpi=300)\n",
        "  # <---- Your code ends here ----->\n",
        "  \n",
        "  print(\"Optimal Hyperparameters Train Accuracy: \", train_acc)\n",
        "  print(\"Optimal Hyperparameters Validation Accuracy: \", val_acc)\n",
        "  print(\"Optimal Hyperparameters Test Accuracy: \", test_acc)\n",
        "  \n",
        "  print(\"Training Scores per Max Leaf Node Count:\", leaf_count_train_scores)\n",
        "  print(\"Validation Scores per Max Leaf Node Count:\", leaf_count_val_scores)\n",
        "\n",
        "  \n",
        "  \n",
        "  return (train_acc, val_acc, test_acc, leaf_count_train_scores, leaf_count_val_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0BXjFYN3SCa"
      },
      "source": [
        "Run the code above for the small dataset and by uncommenting code below. Report all necessary values and both graphs on Latex."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLJCMcNWXCwD",
        "outputId": "e090a161-4dac-48d6-e37f-340550d9915f"
      },
      "source": [
        "retrain_decision_tree(X_train_small, y_train_small, X_val, y_val, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen Depth:  6\n",
            "Chosen Leaf:  9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Hyperparameters Train Accuracy:  0.8159722222222222\n",
            "Optimal Hyperparameters Validation Accuracy:  0.7739130434782608\n",
            "Optimal Hyperparameters Test Accuracy:  0.7142857142857143\n",
            "Training Scores per Max Leaf Node Count: [0.7743055555555556, 0.7743055555555556, 0.7743055555555556, 0.7743055555555556, 0.7743055555555556, 0.7777777777777778, 0.7951388888888888, 0.8159722222222222, 0.8159722222222222]\n",
            "Validation Scores per Max Leaf Node Count: [0.7478260869565218, 0.7478260869565218, 0.7478260869565218, 0.7478260869565218, 0.7478260869565218, 0.7478260869565218, 0.7130434782608696, 0.7739130434782608, 0.7739130434782608]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8159722222222222,\n",
              " 0.7739130434782608,\n",
              " 0.7142857142857143,\n",
              " [0.7743055555555556,\n",
              "  0.7743055555555556,\n",
              "  0.7743055555555556,\n",
              "  0.7743055555555556,\n",
              "  0.7743055555555556,\n",
              "  0.7777777777777778,\n",
              "  0.7951388888888888,\n",
              "  0.8159722222222222,\n",
              "  0.8159722222222222],\n",
              " [0.7478260869565218,\n",
              "  0.7478260869565218,\n",
              "  0.7478260869565218,\n",
              "  0.7478260869565218,\n",
              "  0.7478260869565218,\n",
              "  0.7478260869565218,\n",
              "  0.7130434782608696,\n",
              "  0.7739130434782608,\n",
              "  0.7739130434782608])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0-UXUer3SCd"
      },
      "source": [
        "### Question 5.3 Report on LaTeX\n",
        "Answer the following question on LaTeX in the respective section.\n",
        "1. Report the values on LaTeX.\n",
        "2. How did the training accuracy and testing accuracy change after tuning compared to before? Briefly explain why.\n",
        "3. Paste the plot and explain any trends or patterns with the plot within validation and training scores and briefly explain why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbNi5JkX0HnG"
      },
      "source": [
        "## Additional Exercise (Ungraded)\n",
        "\n",
        "This section is ungraded. You can try it out for fun. sklearn's [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) used in this homework has a parameter called **min_samples_split**. Try playing around with it's values to see if you can get better accuracy for the X_train_small dataset. \n",
        "\n",
        "You don't need to add results of this section in your submission. It is for you to try out. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2xmO04DXCwF"
      },
      "source": [
        "# Question 6: Feature Scaling Effects on KNNs and DTs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG7pZLBoXCwG"
      },
      "source": [
        "### Observing effects of standardizing features\n",
        "\n",
        "Up until now, we have not been using standardized features. Let's observe the effects of standardized features with decision trees and KNNs.\n",
        "\n",
        "Standardization, or feature scaling / data normalization, is a common preprocessing step for data within machine learning. We will see why it's important.\n",
        "\n",
        "Here is a definition taken from SK-Learn's website on Standardization:\n",
        "\n",
        "*Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.*\n",
        "\n",
        "*In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.*\n",
        "\n",
        "Learn More: https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "\n",
        "To start, uncomment the code below and run to retrieve the data. (Recomment before submission.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5RokryGXCwH"
      },
      "source": [
        "# We will use the same data as the previous tasks\n",
        "# Normalize data\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_val_scaled = scaler.transform(X_val)\n",
        "# X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9COC_Pa3SCg"
      },
      "source": [
        "### Helper Functions\n",
        "  \n",
        "We implemented above the KNN algorithm. Sci-kit learn also has their own version of the KNN algorithm which we will use in this following task. Use the two helper functions below in this next task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJvoFpqh3SCg"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def train_KNN(X, y, norm=2, K=5):\n",
        "  \"\"\"\n",
        "  Trains a KNN classifier on the given X, y data with the specified \n",
        "  norm and K.\n",
        "  \n",
        "  Args:\n",
        "    X ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by\n",
        "                          p (number of features) matrix\n",
        "    y ((n,) np.ndarray): The input ys, which are in an n length array\n",
        "    norm (int): The number form of the norm. Note that sklearn only allows L1 and L2 norms,\n",
        "                (norm would be 1 and 2 respectively). Default is 2.\n",
        "    K (int): The value of K for the KNN algorithm. Default is 5.\n",
        "  \n",
        "  Returns:\n",
        "    clf(KNeighborsClassifier): the trained KNN model\n",
        "  \"\"\"\n",
        "  \n",
        "  clf = KNeighborsClassifier(n_neighbors=K, p=norm)\n",
        "  clf.fit(X,y)\n",
        "  return clf\n",
        "\n",
        "def train_decision_tree(X, y, depth=None, leaf_count=None):\n",
        "  \"\"\"\n",
        "  This helper function is defined again from a previous section. \n",
        "  \n",
        "  Trains a decision tree classifier on the given X, y data with the specified \n",
        "  tree depth d and max leaf node count max_leaf_num.\n",
        "  \n",
        "  Args:\n",
        "    X ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by\n",
        "                          p (number of features) matrix\n",
        "    y ((n,) np.ndarray): The input ys, which are in an n length array\n",
        "    depth (int): The maximum depth of the tree. A value of None means no restrictions\n",
        "             on the depth of the tree.\n",
        "    leaf_count (int): The maximum leaf count of the tree's leaf nodes. A value of None means \n",
        "    no restrictions on the leaf count of the tree.\n",
        "  \n",
        "  Returns:\n",
        "    clf(DecisionTreeClassifier): the trained decision tree\n",
        "  \"\"\"\n",
        "  clf = DecisionTreeClassifier(max_depth=depth, max_leaf_nodes=leaf_count, criterion=\"entropy\", random_state=1)\n",
        "  clf.fit(X,y)\n",
        "  return clf\n",
        "\n",
        "def predict(clf, X_test):\n",
        "  \"\"\"\n",
        "  This helper function is defined again from a previous section. \n",
        "  \n",
        "  Uses a trained model to predict on a given test set.\n",
        "  \n",
        "  Args:\n",
        "    clf (Classifier): Trained classifier such as KNN or Decision Tree\n",
        "    X_test ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by\n",
        "                               p (number of features) matrix\n",
        "  \n",
        "  Returns:\n",
        "    y_pred ((n,) np.ndarray): The output predictions, which are in an n length array\n",
        "  \"\"\"\n",
        "  y_pred = clf.predict(X_test)\n",
        "  return y_pred\n",
        "\n",
        "def evaluate(predicted_values, actual_values):\n",
        "    \"\"\"\n",
        "    This helper function is defined again from a previous section. \n",
        "    \n",
        "    Computes the accuracy of the given datapoints.\n",
        "    \n",
        "    Args:\n",
        "        predicted_values: numpy array\n",
        "        actual_values: numpy array\n",
        "    \n",
        "    Returns:\n",
        "        a floating point number representing the accuracy\n",
        "    \"\"\"\n",
        "    return accuracy_score(predicted_values, actual_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVfyIhUiXCwK"
      },
      "source": [
        "### Retrieving Metrics for Unstandardized Data\n",
        "Fill out this function to retrieve training and test accuracies for both KNN and decision tree models. Use default hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0XxTvHMXCwM"
      },
      "source": [
        "def get_classifier_metrics(X_train, y_train, X_test, y_test):\n",
        "  \"\"\"\n",
        "  Create a decision tree and KNN classifer on the normal dataset.\n",
        "    \n",
        "  Args: (Note that n is not the same among train and test sets, \n",
        "         but merely refers to sample size)\n",
        "    X_train ((n,p) np.ndarray)\n",
        "    y_train ((n,) np.ndarray)\n",
        "    X_test ((n,p) np.ndarray)\n",
        "    y_test ((n,) np.ndarray)\n",
        "\n",
        "  To return:\n",
        "    knn_train_accuracy (float): Accuracy of KNN for train set\n",
        "    knn_test_accuracy (float): Accuracy of KNN for test set\n",
        "    dt_train_accuracy (float): Accuracy of DT for train set\n",
        "    dt_test_accuracy (float): Accuracy of DT for test set    \n",
        "  \"\"\"\n",
        "  \n",
        "  # <---- Your code here ----->\n",
        "  clf = train_KNN(X_train, y_train)\n",
        "  knn_train_accuracy = evaluate(predict(clf, X_train), y_train)\n",
        "  knn_test_accuracy = evaluate(predict(clf, X_test), y_test)\n",
        "\n",
        "  DTclf = train_decision_tree(X_train, y_train)\n",
        "  dt_train_accuracy = evaluate(predict(DTclf, X_train), y_train)\n",
        "  dt_test_accuracy = evaluate(predict(DTclf, X_test), y_test)\n",
        "\n",
        "  # <---- Your code ends here ----->\n",
        "  \n",
        "  print(\"knn_train_accuracy: \", knn_train_accuracy)\n",
        "  print(\"knn_test_accuracy: \", knn_test_accuracy)\n",
        "  print(\"dt_train_accuracy: \", dt_train_accuracy)\n",
        "  print(\"dt_test_accuracy: \", dt_test_accuracy)\n",
        "  \n",
        "  return knn_train_accuracy, knn_test_accuracy, dt_train_accuracy, dt_test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF3Jmuy53SCk"
      },
      "source": [
        "Uncomment the code below and run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWS33o6OXCwO",
        "outputId": "4e2fa416-3e58-44be-83d6-370459154928"
      },
      "source": [
        "print(\"FOR UNSTANDARDIZED DATA\")\n",
        "get_classifier_metrics(X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"\\nFOR STANDARDIZED DATA\")\n",
        "get_classifier_metrics(X_train_scaled, y_train, X_test_scaled, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOR UNSTANDARDIZED DATA\n",
            "knn_train_accuracy:  0.7899305555555556\n",
            "knn_test_accuracy:  0.7012987012987013\n",
            "dt_train_accuracy:  1.0\n",
            "dt_test_accuracy:  0.7532467532467533\n",
            "\n",
            "FOR STANDARDIZED DATA\n",
            "knn_train_accuracy:  0.8177083333333334\n",
            "knn_test_accuracy:  0.8181818181818182\n",
            "dt_train_accuracy:  1.0\n",
            "dt_test_accuracy:  0.7532467532467533\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8177083333333334, 0.8181818181818182, 1.0, 0.7532467532467533)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZCmPf0G3SCn"
      },
      "source": [
        "### Question 6 Report on LaTex\n",
        "Answer the following question on LaTeX in the respective section.\n",
        "1. Report the values on LaTeX.\n",
        "2. What happens to performance when we use standardization for data with decision trees? What about KNN? Briefly explain why each happened."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-7QVbownVl4"
      },
      "source": [
        "# Submission\n",
        "\n",
        "**Due Sunday 20th September, 11:59pm**\n",
        "\n",
        "**Submit the hw1.ipynb file to gradescope.**\n",
        "\n",
        "To download the .ipynb version go to File->Download as .ipynb\n",
        "\n",
        "\n",
        "Retain the outputs generated by the cells when you upload the notebook.\n",
        "\n",
        "\n",
        "If you are working in pairs make sure to add your team memberâ€™s name on Gradescope when submitting\n"
      ]
    }
  ]
}